{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Early lessons from fast.ai, Part I\n",
    "\n",
    "Clayton Yochum - Methods Consultants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Not the droids you're looking for\n",
    "\n",
    "I'm not going to cover\n",
    "- math of deep learning\n",
    "- DL for structured problems (e.g. timeseries)\n",
    "- NLP anything\n",
    "- embeddings (this should probably be it's own talk at some point)\n",
    "- how to implement any of this (minimal code)\n",
    "\n",
    "![obi-wan](img/droids.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is fast.ai?\n",
    "\n",
    "Umbrella group for courses taught by Jeremy Howard & Rachel Thomas\n",
    "- super smart people, lots of great content on their blog and twitter feeds\n",
    "\n",
    "I took their \"Deep Learning for Coders, Part 1, v2\" last fall\n",
    "- easily the best class on this stuff\n",
    "- only pre-reqs are 1 year of programming experience and high school algebra\n",
    "\n",
    "Other courses:\n",
    "- machine learning (possibly incomplete)\n",
    "- computational linear algebra\n",
    "\n",
    "Heavy emphasis on top-down approach to teaching\n",
    "- do cool stuff _first_, unravel details as course progresses\n",
    "- the opposite how almost all traditional schooling operates\n",
    "- increasingly popular paradigm in data science and technology\n",
    "- I love it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is deep learning?\n",
    "\n",
    "I'm not going to cover the mechanics, but at a high level, DL models\n",
    "- are just big, fancy machine learning models with a ton of parameters\n",
    "    - multi-layer neural networks\n",
    "- have led to many breakthroughs in last ~5 years, including\n",
    "    - image recognition\n",
    "    - language translation\n",
    "    - game playing\n",
    "    - tons of other areas\n",
    "- come in many different forms\n",
    "    - fully connected (DNN/ANN)\n",
    "    - CNN (**C**onvolutional, common for images)\n",
    "    - RNN (**R**ecurrent, common for language/text understanding)\n",
    "    - many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning is (was?) hard\n",
    "\n",
    "Neural networks are an old idea, but deep versions didn't catch on until recently because now we have\n",
    "- lots of (labeled) data\n",
    "- lots of compute (GPUs!)\n",
    "- better software (CUDA/CUDNN, Caffe/Torch/Tensorflow/Keras/PyTorch/etc)\n",
    "- better algorithms (optimizers, efficient batching, network types)\n",
    "\n",
    "Still, it's _hard_ to build a well-performing DL model\n",
    "- many complex, interconnected hyperparameters to consider\n",
    "    - architecture is hard; not just \"tune these 10 knobs\"\n",
    "    - DL is defined as layers or DAG's with nearly endless possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "fast.ai tries to make this as easy as possible\n",
    "- top-down approach\n",
    "    - built a near-perfect image classifier during first lesson\n",
    "- emphasis on tools\n",
    "    - python, jupyter, numpy, tmux, remote linux servers w/ GPUs (AWS, Crestle, Paperspace)\n",
    "- new `fastai` python library\n",
    "    - like Keras, but for PyTorch (high-level abstraction)\n",
    "    - built-in techniques not available anywhere else (yet)\n",
    "    - 1 command in PyTorch tends to be 3-5 in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trix are for Kids\n",
    "\n",
    "![not rabbits](img/trix-rabbit.jpg)\n",
    "\n",
    "...and also deep learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Classification is now easy\n",
    "\n",
    "(for simple problems, at least; domains like medical imaging get a bit trickier)\n",
    "\n",
    "We finally have enough techniques, tools, and tricks that combine to make it easy to get great results:\n",
    "- pre-trained networks\n",
    "- data augmentation\n",
    "- learning rate annealing\n",
    "- SGDR: stochastic gradient descent _with restarts_ (new-ish)\n",
    "- learning rate finders (new)\n",
    "- differential learning rates (new)\n",
    "- easily repeatable workflow process (new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The power of pre-trained networks\n",
    "\n",
    "Computer vision is the most obvious place where deep learning excels\n",
    "- \"is this a cat or a dog?\"\n",
    "\n",
    "This is largely due to using _pre-trained networks_, where someone else built a network for a different image task\n",
    "- most common are networks trained on ImageNet, a collection of images where each belongs to one of 1000 classes like \"cat\", \"dog\", \"tree\", \"car\", etc.\n",
    "\n",
    "This works because neural networks learn high-level feature representations of their inputs which also make sense for other inputs\n",
    "- early layers learn to find lines, curves, dots\n",
    "- then shapes (circles, squares), patterns (grid, honeycomb), and more (eyes, tires)\n",
    "- we know this because of **de**convolutional neural nets: https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So even though a network may have been trained to do 1000-class single-label classification, I can repurpose it to do things like\n",
    "- distinguish cats from dogs\n",
    "- determine breeds of dogs\n",
    "- label satellite images with one or more of hazy, grass, stream, etc.\n",
    "\n",
    "Re-purposing means replacing the final layer from the pre-trained model with one that corresponds to our problem\n",
    "- e.g. 2-node softmax to distinguish cats from dogs\n",
    "\n",
    "Pre-trained networks aren't _just_ for images, but it's a lot easier here\n",
    "- two random images are much more similar than e.g. two random audio snippets\n",
    "- word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Deep learning does better the more data it has, but data collection is hard/expensive. One way to get more data is to modify your existing data to create new examples!\n",
    "\n",
    "For each input image, you can rotate, zoom, crop, rotate, etc. each image to get a slightly different version\n",
    "- chosen transforms will depend on the kinds of images you have\n",
    "     - for side-on images of animals, we might do horizontal flips, but not rotations\n",
    "     - for top-down images you might flip on both axes, rotate +-180\n",
    "\n",
    "Thus we can easily double, quadruple, etc. our training set, and make it more robust to different input perturbations\n",
    "\n",
    "There's also Test-Time Augmentation (TTA): for each test image, transform it a few times, predict each, average the predictions\n",
    "\n",
    "fast.ai encourages and enables both types of augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rates\n",
    "\n",
    "If you're using a pre-trained model, you don't have to worry about most hyperparameters, since you're using a network topology chosen by someone else.\n",
    "\n",
    "But there's still one big one: the learning rate.\n",
    "\n",
    "Most deep nets learn by updating their parameters via _gradient descent_ and _backpropogation_ (chain rule). We run some images through a model, get some predictions, compare those to the correct answers, then calculate a gradient to point us in a directions that will give us _better_ parameters. The learning rate is _how big of a step we take_.\n",
    "\n",
    "If your learning rate is _too low_, it'll take forever to get a well-trained model.\n",
    "If you learning rate is _too big_, you'll jump all around your param space and never find a nice local minima\n",
    "\n",
    "Setting appropriate learning rates is the kind of thing that can drive users insane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LR Annealing\n",
    "\n",
    "One technique that's been in common use for some time is _learning rate annealing_: decrease the learning rate as training progresses.\n",
    "\n",
    "Start with big steps, take smaller and smaller steps as you get closer and closer to a good answer.\n",
    "\n",
    "_Annealing_ here is like _simulated annealing_, another optimization technique\n",
    "- swap \"temperature\" (SA) for \"learning rate\" (SGD)\n",
    "- comes from metallurgy!\n",
    "\n",
    "The learning rate is usually updated after each _batch_ (next slide), and fast.ai uses a cosine function to do this (as opposed to e.g. a straight line); _cosine annealing_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGDR\n",
    "\n",
    "_Stochastic Gradient Descent with Restarts_\n",
    "\n",
    "We train on _batches_ of inputs (images) at a time, where we make predictions with the whole batch for a given set of parameters, then use all those predictions to update our weights via gradient descent.\n",
    "\n",
    "So \"stochastic\" just means we're updating incrementally, rather than pushing all our training examples through before updating our parameters. _Incremental_ gradient descent.\n",
    "\n",
    "(We still take multiple passes through our inputs, usually; one use of all inputs is called an _epoch_)\n",
    "\n",
    "The \"R\" is a bit of a newer technique: we anneal our learning rate over the course of one of more epochs (make it smaller each batch), but then jump back up to our original rate, and repeat training and annealing for additional epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SGDR\n",
    "\n",
    "Our learning rate ends up in patterns like this:\n",
    "\n",
    "![fixed](img/sgdr_constant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "or this:\n",
    "\n",
    "![cycle_mult](img/sgdr_cyclical.png)\n",
    "\n",
    "There's another cool image in this paper on Snapshot ensembles: https://arxiv.org/pdf/1704.00109.pdf\n",
    "- with a snapshot ensemble, you save the weights at the end of each annealing cycle, and average predictions from each\n",
    "    - not something we've done in fast.ai, but can be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rate Finder\n",
    "\n",
    "So learning rates are important, and it's important to change them during training, but it's still important to start them in a good place. How are we supposed to do that?\n",
    "\n",
    "Jeremy discovered a trick buried in a paper about something else, and used it to build a _learning rate finder_ into fast.ai.\n",
    "\n",
    "Surprisingly simple: start the learning rate very low, and do up-to-an-epoch of training, increasing the learning rate exponentially between batches. Stop when loss is getting way worse or the epoch over.\n",
    "\n",
    "Then we start with (roughly) as high a learning-rate as possible, where the loss is still clearly improving (getting lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate Finder\n",
    "\n",
    "So the learning rate looks like this during the course of \"training\" (weight adjustments during learning rate finding are not persisted):\n",
    "\n",
    "![lr vs time](img/sched-plot-lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we get a plot like\n",
    "\n",
    "![loss vs lr](img/sched-plot-loss.png)\n",
    "\n",
    "And in this case would _start_ our learning rate around 1e-2\n",
    "- why not 1e-1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differential Learning Rates\n",
    "\n",
    "Yet another trick not widely in use outside of fast.ai is the concept of _differential learning rates_.\n",
    "\n",
    "To get world-class performance, we generally train a model several times in a handful of different way. When we start, we're only tuning the final layer, the part specific to our problem, assuming the pre-trained layers are already pretty good.\n",
    "\n",
    "Sometimes it makes sense to further tune the earlier layers, particularly if our images are substantially different from the ones the pre-trained model learned from.\n",
    "\n",
    "However, these layers shouldn't need as much tuning as our last layer, so we want _smaller_ learning rates for earlier layers. Even among the pre-trained layers, we want to change the earlier layers less than the later ones. Thus we want differential learning rates, where different groups of layers learn at different rates.\n",
    "\n",
    "Apparently this is possible in other DL frameworks, but it's a little easier in fast.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unintutive High Dimensional Behavior\n",
    "\n",
    "I really latched on to a minor aside from Jeremy in one of the first lessons:\n",
    "> most local minima are roughly equivalent to both themselves and the global minima\n",
    "\n",
    "If this is true, it doesn't really matter what local minima we're in, and it's a waste of time trying to find the global min!\n",
    "\n",
    "The relevant paper is from January 2015: https://arxiv.org/pdf/1412.0233.pdf\n",
    "\n",
    "It's dense (compares DNN's to Hamiltonian of spherical spin-glass models...), but also kinda mind-blowing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Several big assumptions:\n",
    "- variable independence\n",
    "- redundancy of network parameterization\n",
    "- uniformity\n",
    "\n",
    "The basic gist is there's both theoretical and practical support for the notion that in sufficiently large networks, there's a \"band\" full of pesky saddle-points, but beyond that a band where critical points are nearly all high-quality local minima (increasingly true as network sizes increase).\n",
    "\n",
    "This is why training a model with an order of magnitude more parameters than data somehow works!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
